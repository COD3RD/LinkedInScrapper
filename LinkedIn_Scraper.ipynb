{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original Script by ChatGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from openpyxl import Workbook\n",
    "\n",
    "def scrape_linkedin_jobs(search_query, max_pages):\n",
    "    base_url = \"https://www.linkedin.com/jobs/search/\"\n",
    "    jobs = []\n",
    "    \n",
    "    for page in range(1, max_pages + 1):\n",
    "        params = {\n",
    "            \"keywords\": search_query,\n",
    "            \"start\": (page - 1) * 25,\n",
    "        }\n",
    "        response = requests.get(base_url, params=params)\n",
    "        #print(response.content)\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "            job_cards = soup.find_all(\"li\", class_=\"result-card\")\n",
    "            \n",
    "            for job_card in job_cards:\n",
    "                job_title = job_card.find(\"h3\", class_=\"result-card__title\").get_text().strip()\n",
    "                company = job_card.find(\"h4\", class_=\"result-card__subtitle\").get_text().strip()\n",
    "                job_link = job_card.find(\"a\", class_=\"result-card__full-card-link\")[\"href\"]\n",
    "                jobs.append({\"title\": job_title, \"company\": company, \"link\": job_link})\n",
    "        \n",
    "    return jobs\n",
    "\n",
    "def save_to_spreadsheet(jobs, filename):\n",
    "    wb = Workbook()\n",
    "    ws = wb.active\n",
    "    ws.append([\"Job Title\", \"Company\", \"Job Link\"])\n",
    "    \n",
    "    for job in jobs:\n",
    "        ws.append([job[\"title\"], job[\"company\"], job[\"link\"]])\n",
    "    \n",
    "    wb.save(filename)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    search_query = \"Data Governance\"\n",
    "    max_pages = 2  # Number of pages to scrape (each page has 25 jobs)\n",
    "    filename = \"linkedin_jobs.xlsx\"\n",
    "    \n",
    "    scraped_jobs = scrape_linkedin_jobs(search_query, max_pages)\n",
    "    save_to_spreadsheet(scraped_jobs, filename)\n",
    "    print(f\"Scraped {len(scraped_jobs)} jobs and saved to {filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Script adjusted based on current response from LinkedIn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from openpyxl import Workbook\n",
    "\n",
    "def scrape_linkedin_jobs(search_query, max_pages):\n",
    "    base_url = \"https://www.linkedin.com/jobs/search/\"\n",
    "    jobs = []\n",
    "    \n",
    "    for page in range(2, max_pages + 1):\n",
    "        params = {\n",
    "            \"keywords\": search_query,\n",
    "            \"start\": (page - 1) * 25,\n",
    "        }\n",
    "        response = requests.get(base_url, params=params)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "            #print(soup)\n",
    "            job_cards = soup.find_all(\"div\", class_=\"base-card relative w-full hover:no-underline focus:no-underline base-card--link base-search-card base-search-card--link job-search-card\")\n",
    "            print(\"Job Cards are \",job_cards)\n",
    "            for job_card in job_cards:\n",
    "                job_title = job_card.find(\"h3\", class_=\"base-search-card__title\").get_text().strip()\n",
    "                company = job_card.find(\"h4\", class_=\"base-search-card__subtitle\").get_text().strip()\n",
    "                job_link = job_card.find(\"a\", class_=\"base-card__full-link absolute top-0 right-0 bottom-0 left-0 p-0 z-[2]\")[\"href\"]\n",
    "                \n",
    "                # Visit the job link to get the description\n",
    "                job_description = get_job_description(job_link)\n",
    "                print(job_title)\n",
    "                jobs.append({\n",
    "                    \"title\": job_title,\n",
    "                    \"company\": company,\n",
    "                    \"link\": job_link,\n",
    "                    \"description\": job_description\n",
    "                })\n",
    "        \n",
    "    return jobs\n",
    "\n",
    "def get_job_description(job_link):\n",
    "    response = requests.get(job_link)\n",
    "    print(response.content)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        description = soup.find(\"div\", class_=\"show-more-less-html__markup\").get_text()\n",
    "        return description.strip() if description else \"N/A\"\n",
    "    else:\n",
    "        return \"N/A\"\n",
    "\n",
    "def save_to_spreadsheet(jobs, filename):\n",
    "    wb = Workbook()\n",
    "    ws = wb.active\n",
    "    ws.append([\"Job Title\", \"Company\", \"Job Link\", \"Job Description\"])\n",
    "    \n",
    "    for job in jobs:\n",
    "        ws.append([job[\"title\"], job[\"company\"], job[\"link\"], job[\"description\"]])\n",
    "    \n",
    "    wb.save(filename)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    search_query = \"Data Governance Manager\"     #Change the Keyword here. \n",
    "    max_pages = 10  # Number of pages to scrape (each page has 25 jobs)  \n",
    "    filename = \"linkedin_jobs.xlsx\"\n",
    "    \n",
    "    scraped_jobs = scrape_linkedin_jobs(search_query, max_pages)\n",
    "    save_to_spreadsheet(scraped_jobs, filename)\n",
    "    print(f\"Scraped {len(scraped_jobs)} jobs and saved to {filename}\")\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
